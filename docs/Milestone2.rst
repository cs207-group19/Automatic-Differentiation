{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 207 Final Project: Milestone 2\n",
    "### Group 19: Chen Shi, Stephen Slater, Yue Sun\n",
    "### November 2018\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Differentiation, i.e. finding derivatives, has long been one of the key operations in computation related to modern science and engineering. In optimization and numerical differential equations, finding the extrema will require differentiation. There are many important applications of automatic differentiation in optimization, machine learning, and numerical methods  (e.g., time integration, root-finding). This software library  will use the concept of automatic differentiation to solve differentiation problems in scientific computing.\n",
    "\n",
    "### Background\n",
    "The chain rule, gradient (Jacobian), computational graph, elementary functions and several numerical methods serve as the mathematical cornerstone for this software. The mathematical concepts here come from CS 207 Lectures 9 and 10 on Autodifferentiation.\n",
    "\n",
    "### The Chain Rule\n",
    "\n",
    "The chain rule is critical to AD, since the derivative of the function with respect to the input is dependent upon the derivative of each trace in the evaluation with respect to the input.\n",
    "   \n",
    "If we have  $h(u(x))$ then the derivative of $h$ with respect to $x$ is:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial h}{\\partial x} =\\frac{\\partial h}{\\partial u} \\cdot \\frac{\\partial u}{\\partial x}\n",
    "\\end{equation}\n",
    "\n",
    "If we have another argument $h(u, v)$ where $u$ and $v$ are both functions of $x$, then the derivative of $h(x)$ with respect to $x$ is:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial h}{\\partial x} =\\frac{\\partial h}{\\partial u} \\cdot \\frac{\\partial u}{\\partial x} + \\frac{\\partial h}{\\partial v} \\cdot \\frac{\\partial v}{\\partial x}\n",
    "\\end{equation}\n",
    "\n",
    "### Gradient and Jacobian\n",
    "\n",
    "If we have $x\\in\\mathbb{R}^{m}$ and function $h\\left(u\\left(x\\right),v\\left(x\\right)\\right)$, we want to calculate the gradient of $h$ with respect to $x$:\n",
    "\\begin{equation}\n",
    "  \\nabla_{x} h = \\frac{\\partial h}{\\partial u}\\nabla_x u + \\frac{\\partial h}{\\partial v} \\nabla_x v\n",
    "\\end{equation}\n",
    "\n",
    "\\noindent In the case where we have a function $h(x): \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$, we write the Jacobian matrix as follows, allowing us to store the gradient of each output with respect to each input.\n",
    "\n",
    "$$\n",
    "\\textbf{J} =\n",
    "\\begin{bmatrix}\n",
    "  \\frac{\\partial h_1}{\\partial x_1} & \n",
    "    \\frac{\\partial h_1}{\\partial x_2} & \\ldots &\n",
    "    \\frac{\\partial h_1}{\\partial x_m} \\\\[1ex] \n",
    "  \\frac{\\partial h_2}{\\partial x_1} & \n",
    "    \\frac{\\partial h_2}{\\partial x_2} & \\ldots &\n",
    "    \\frac{\\partial h_2}{\\partial x_m} \\\\[1ex]\n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\[1ex]\n",
    "  \\frac{\\partial h_n}{\\partial x_1} & \n",
    "    \\frac{\\partial h_n}{\\partial x_2} & \\ldots &\n",
    "    \\frac{\\partial h_n}{\\partial x_m}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "In general, if we have a function $g\\left(y\\left(x\\right)\\right)$ where $y\\in\\mathbb{R}^{n}$ and $x\\in\\mathbb{R}^{m}$.  Then $g$ is a function of possibly $n$ other functions, each of which can be a function of $m$ variables.  The gradient of $g$ is now given by \n",
    "\\begin{equation}\n",
    "  \\nabla_{x}g = \\sum_{i=1}^{n}{\\frac{\\partial g}{\\partial y_{i}}\\nabla_x y_{i}\\left(x\\right)}.\n",
    "\\end{equation}\n",
    "\n",
    "### The Computational Graph\n",
    "The computational graph lets us visualize what happens during the evaluation trace. The following example is based on Lectures 9 and 10. Consider the function:\n",
    "$$f\\left(x\\right) = x - \\exp\\left(-2\\sin^{2}\\left(4x\\right)\\right)$$ \n",
    "If we want to evaluate $f$ at the point $x$, we construct a graph where the input value is $x$ and the output is $y$. Each input variable is a node, and each subsequent operation of the execution trace applies an operation to one or more previous nodes (and creates a node for constants when applicable).\n",
    "\n",
    "As we execute $f(x)$ in the \"forward mode\", we can propagate not only the sequential evaluations of operations in the graph given previous nodes, but also the derivatives using the chain rule.\n",
    "\n",
    "Let's find $f\\left(\\dfrac{\\pi}{16}\\right)$.  The evaluation trace looks like:\n",
    "\n",
    "\n",
    "\\begin{table}[!htbp]\n",
    "\\centering\n",
    "\\begin{tabular}{1c1c1c1}\\\\\n",
    "& Trace    & Elementary Operation                  & Numerical Value                \\\\ [3pt]\n",
    "& $x_{1}$  & $\\frac{\\pi}{16}$                      & $\\frac{\\pi}{16}$                \\\\[3pt]\n",
    "& $x_{2}$  & $4x_{1}$                           & $\\frac{\\pi}{4}$                 \\\\[3pt]\n",
    "& $x_{3}$  & $\\sin\\left(x_{2}\\right)$               & $\\frac{\\sqrt{2}}{2}$            \\\\[3pt]\n",
    "& $x_{4}$  & $x_{3}^{2}$                            & $\\frac{1}{2}$                   \\\\[3pt]\n",
    "& $x_{5}$  & $-2x_{4}$                              & $-1$                             \\\\[3pt]\n",
    "& $x_{6}$  & $\\exp\\left(x_{5}\\right)$               & $\\frac{1}{e}$                   \\\\[3pt]\n",
    "& $x_{7}$  & $-x_{6}$                               & $-\\frac{1}{e}$                  \\\\[3pt]\n",
    "& $x_{8}$  & $x_{1} + x_{7}$                        & $\\frac{\\pi}{16} - \\frac{1}{e}$ \n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "The computer holds floating point values.  The value of $f\\left(\\dfrac{\\pi}{16}\\right)$ is $-0.17152990032208026$.\n",
    "\n",
    "\n",
    "### Elementary functions\n",
    "\n",
    "An elementary function is built up of a finite combination of constant functions, field operations $(+, -, \\times, \\div)$, algebraic, exponential, trigonometric, hyperbolic and logarithmic functions and their inverses under repeated compositions. Below is a table of some elementary functions and examples that we will include in our implementation.\n",
    "\\begin{table}[!htbp]\n",
    "\\centering\n",
    "\\begin{tabular}{1c1c1c1} \\\\\n",
    "&Elementary Functions  & Example \\\\ [3pt]\n",
    "&powers &x^2 \\\\ [3pt]\n",
    "&roots &\\sqrt{x} \\\\ [3pt]\n",
    "&exponentials &e^{x} \\\\ [3pt]\n",
    "&logarithms &\\log(x) \\\\ [3pt]\n",
    "&trigonometrics &\\sin(x) \\\\ [3pt]\n",
    "&inverse trigonometrics &\\arcsin(x) \\\\ [3pt]\n",
    "&hyperbolics &\\sinh(x)\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "# How to Use *DeriveAlive*\n",
    "\n",
    "The user will use \\texttt{pip} to install the package. After installation, the user will need to import our package (see psuedocode below). Implicitly, this will import other dependencies (such as \\texttt{numpy}), since we include those dependencies as imports in our module. Then, the user will define an input of type \\texttt{Var} in our module. After this, the user can define a function in terms of this \\texttt{Var}. Essentially, the user will give the initial input $x$ and then apply $f$ to $x$ and store the new value and derivative with respect to $x$ inside $f$. At each step of the evaluation, the program will process nodes in the implicit computation graph in order, propagating values and derivatives. The final output yield another \\texttt{Var} containing $f(x)$ and $f'(x)$.\n",
    "\n",
    "For example, consider the case $f(x) = \\sin(x) + 5$. If the user wants to evaluate $f(x)$ at $x = a$, where $a = \\pi/2$, the user will instantiate a \\texttt{Var} object as \\texttt{da.Var(np.pi/2)}. Then, the user will give the initial input $a$ and set $f1 = f(a)$, which stores $f(a)$ and $f'(a)$ as attributes inside the \\texttt{Var} $f1$. This functionality will propagate throughout the graph with more variables in a recursive structure, where each evaluation trace creates a new \\texttt{Var}. See the code below for a demonstration. Note that the $\\sin(\\pi/2) = 1.0$, and the derivative is $0$. Also note that we can assign $f1 = f(a)$ by explicitly applying the operations of $f$ to $a$ without creating an intermediary $f$.\n",
    "\n",
    "\\begin{verbatim}\n",
    "  ## install at command line\n",
    "  pip install DeriveAlive\n",
    "  \n",
    "  ## Python\n",
    "  Python\n",
    "  >>> import DeriveAlive as da\n",
    "  >>> import numpy as np\n",
    "  \n",
    "  # Expect value of 18.42, derivative of 6.0\n",
    "  >>> x1 = da.Var(np.pi / 2)\n",
    "  >>> f1 = 3 * 2 * x1.sin() + 2 * x1 + 4 * x1 + 3\n",
    "  >>> print (f1.val, f1.der)\n",
    "  18.42477796076938 6.0\n",
    "\n",
    "  # Expect value of 0.5, derivative of -0.25\n",
    "  >>> x2 = da.Var(2.0)\n",
    "  >>> f2 = 1 / x2\n",
    "  >>> print (f2.val, f2.der)\n",
    "  0.5 -0.25\n",
    "\n",
    "  # Expect value of 1.5, derivative of 0.5\n",
    "  >>> x3 = da.Var(3.0)\n",
    "  >>> f3 = x3 / 2\n",
    "  >>> print (f3.val, f3.der)\n",
    "  1.5 0.5\n",
    "\\end{verbatim}\n",
    "\n",
    "# Software Organization\n",
    "\\subsection{Current directory structure}\n",
    "\\begin{align*}\n",
    "\\texttt{cs207-FinalProject/} & \\\\\n",
    "& \\texttt{README.md} \\\\\n",
    "& \\texttt{LICENSE} \\\\\n",
    "& \\texttt{DeriveAlive.py} \\\\\n",
    "& \\texttt{docs/} \\\\\n",
    "& \\indent \\:\\:\\:\\:\\:\\texttt{milestone1.pdf} \\\\\n",
    "& \\texttt{tests/} \\\\\n",
    "& \\indent \\:\\:\\:\\:\\:\\texttt{test\\_DeriveAlive.py} \\\\\n",
    "& \\cdots\n",
    "\\end{align*}\n",
    "\n",
    "\\subsection{Included modules and their functionality}\n",
    "\n",
    "\\begin{itemize}\n",
    "    \\item \\texttt{NumPy} - This provides an API for a large collection of high-level mathematical operations. In addition, it provides support for large, multi-dimensional arrays and matrices.\n",
    "    \\item \\texttt{doctest} - This module searches for pieces of text that look like interactive Python sessions (typically within the documentation of a function), and then executes those sessions to verify that they work exactly as shown.\n",
    "    \\item \\texttt{pytest} - This is an alternative, more Pythonic way of writing tests, making it easy to write small tests, yet scales to support complex functional testing. We plan to use this for a comprehensive test suite.\n",
    "    \\item \\texttt{setuptools} - This package allows us to create a package out of our project for easy distribution. See more information on packaging instructions here: \\\\\n",
    "    \\url{https://packaging.python.org/tutorials/packaging-projects/}.\n",
    "\\end{itemize}\n",
    "\n",
    "\\subsection{Where will your test suite live?}\n",
    "Our test suite will be in a test file in its own \\texttt{tests} folder. We will use Travis CI for automatic testing for each push, and Coveralls for line coverage metrics. We have already set up these integrations, with badges included in the \\texttt{README.md}.\n",
    "\n",
    "\\subsection{How will you distribute your package?}\n",
    "We will use Python Package Index (PyPI) for distributing our package. PyPI is the official third-party software repository for Python and primarily hosts Python packages in the form of archives called sdists (source distributions) or precompiled wheels.\n",
    "\n",
    "\\section{Implementation}\n",
    "\n",
    "We plan to implement the forward mode of autodifferentiation with the following choices:\n",
    "\n",
    "\\begin{itemize}\n",
    "    \\item Core data structures: The core data structures will be classes, lists and numpy arrays. \n",
    "    \\begin{itemize}\n",
    "        \\item Classes will help us provide an API for differentiation and custom functions, including custom methods for our elementary functions.\n",
    "        \\item Lists will help us maintain the collection of trace variables and output functions (in the case of multi-output models) from the computation graph in order. For example, if we have a function $f(x): \\mathbb{R}^1 \\rightarrow \\mathbb{R}^2$, then we store $f = [f1, f2]$, where we have defined $f1$ and $f2$ as functions of $x$, and we simply process the functions in order. We will also use lists as a class attribute within each \\texttt{Var} to keep track of the derivatives with respect to each input variable, where the length of the list is the number of input variables in the function. Depending on the extensions we choose for the project, we may use lists to store the parents of each node in the graph.\n",
    "        \\item Numpy arrays are included for operations on the Jacobian, in case we need to store all possible derivatives in one data structure -- this is especially helpful in the multi-input and multi-output function case (vector-vector functions). If we want to optimize our computation, we can store the list of derivatives as a numpy array so that we can apply entire functions to the array, rather than to each entry separately. In the vector-vector case, if we have a function $f: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$, we can process this as $f = [f_1, f_2, \\ldots, f_n]$, where each $f_i$ is a function $f_i: \\mathbb{R}^m \\rightarrow \\mathbb{R}$. Our implementation can act as a wrapper over these functions, and we can evaluate each $f_i$ independently, so long as we define $f_i$ in terms of the $m$ inputs.\n",
    "    \\end{itemize}\n",
    "       \n",
    "    \\item Our implementation plan currently includes 1 class, but as we progress through the project, we are considering changing this to 2 classes for efficiency purposes:\n",
    "    \\begin{itemize}\n",
    "        \\item \\texttt{Var} class. The class instance itself has two main attributes: the value and the evaluated derivatives with respect to each input. Within the class we redefine the elementary functions and basic algebraic functions, including both evaluation and derivation. Since our computation graph includes ``trace\" variables, this class will account for each variable. Similar to a dual number, this class structure will allow us easy access to necessary attributes of each variable, such as the trace evaluation and the evaluated derivative with respect to each input variable. This trace table would also be of possible help in future project extensions. \\\\\n",
    "        \\item \\texttt{Operations} class (maybe). We currently store the operations within each \\texttt{Var} instance, but in order to avoid duplicating these, we may separate the code to include all operations in their own class.\n",
    "    \\end{itemize}\n",
    "    \n",
    "    \\item Methods and class attributes:\n",
    "    \\begin{itemize}\n",
    "        \\item We will overload elementary mathematical operations such as addition, division, sine, etc. that will take in $1$ \\texttt{Var} type, or $2$ \\textt{Var} types, or $1$ \\textt{Var} and $1$ constant, and return a new \\texttt{Var} (i.e. the next ``trace\" variable). All other operations on constants will use the standard Python library. In each \\texttt{Var}, we will store attributes of the value of the variable (which is calculated based on the current operation and previous trace variables) and the evaluated gradient of the variable with respect to each input variable.\n",
    "        \\item Methods in \\texttt{Var}: \\texttt{\\_\\_add\\_\\_}, \\texttt{\\_\\_radd\\_\\_}, \\texttt{\\_\\_mul\\_\\_}, \\texttt{\\_\\_rmul\\_\\_}, \\texttt{\\_\\_truediv\\_\\_}, \\texttt{\\_\\_rtruediv\\_\\_}, \\texttt{\\_\\_sin\\_\\_}, \\texttt{\\_\\_cos\\_\\_}, \\texttt{\\_\\_tan\\_\\_}, \\texttt{\\_\\_pow\\_\\_}, \\texttt{\\_\\_log\\_\\_}, \\texttt{\\_\\_exp\\_\\_} (more to be included)\n",
    "        \\item Attributes in \\texttt{Var}: \\texttt{self.var}, \\texttt{self.der}. To cover the case of multiple inputs, we plan to implement our \\texttt{self.der} as a list, in order to account for derivatives with respect to each input variable.\n",
    "        \n",
    "    \\end{itemize}\n",
    "    \\item External dependencies:\n",
    "    \\begin{itemize}\n",
    "        \\item Modules: \\texttt{NumPy}, \\texttt{doctest}, \\texttt{pytest}, \\textt{setuptools}\n",
    "        \\item Test suites: Travis CI, Coveralls\n",
    "    \\end{itemize}\n",
    "    \\item How will we deal with elementary functions like $\\sin$ and $\\exp$?\n",
    "    \\begin{itemize}\n",
    "        \\item To handle these functions, we will define our own custom elementary functions within the \\texttt{Var} class so that we can calculate the $\\sin(x)$ of a variable $x$ using a package such as \\texttt{numpy}, and also store the proper gradient ($-\\cos(x)dx$) to propagate the gradients forward. For example, consider a univariate function where \\texttt{self.val} contains the current evaluation trace and \\texttt{self.der} is a numpy array of the derivative of the current trace with respect to the input. When we apply $\\sin$, we propagate as follows:\n",
    "        \\begin{verbatim} def sin(self):\n",
    "    \t\tval = np.sin(self.val)\n",
    "    \t\tder = np.cos(self.val) * self.der\n",
    "    \t\treturn Var(val, der)\n",
    "        \\end{verbatim}\n",
    "    \\end{itemize}\n",
    "\\end{itemize}\n",
    "\\noindent \\section{Sources}\n",
    "\\begin{itemize}\n",
    "    \\item CS 207 Lectures 9 and 10 (Autodifferentiation) \n",
    "    \\item Elementary functions: \\url{https://en.wikipedia.org/wiki/Elementary_function}\n",
    "    \\item Package distribution: \\url{https://packaging.python.org/tutorials/packaging-projects/}\n",
    "\\end{itemize}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
